# -*- coding: utf-8 -*-
"""Q2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ka9k6ZT90IzWwL1W4JUErkgLjSSlBgkU
"""

'''
Q2: Generate Data for Linear Regression using w1 and w2 and the Estimate the error
'''
import numpy as np
import matplotlib.pyplot as plt

class TwoModelDataGenerator:
    """
    Generates data using one of two orthogonal vectors w1 and w2.
    """
    def __init__(self, N, D, p1 = 0.9, noise_level = 1.0):
        self.N = N
        self.D = D
        self.p1 = p1
        self.p2 = 1 - p1
        self.noise_level = noise_level
        self._generate_orthogonal_weights()

    def _generate_orthogonal_weights(self):
        # W1
        w1_true = np.random.randn(self.D + 1)

        # W2
        w2_initial = np.random.randn(self.D + 1)
        # proj_w1(w2_initial) = (w2_initial . w1_true) / ||w1_true||^2 * w1_true
        projection_on_w1 = (np.dot(w2_initial, w1_true) / np.dot(w1_true, w1_true)) * w1_true
        w2_true = w2_initial - projection_on_w1

        self.w1_true = w1_true
        self.w2_true = w2_true

    def _generate_features(self):
        X = np.random.randn(self.N, self.D)
        return X

    def _generate_labels(self, X):
        X_c = np.c_[np.ones(self.N), X]

        model_choice = np.random.rand(self.N)
        w1_mask = model_choice <= self.p1
        assignments = w1_mask.astype(int)

        y = np.zeros(self.N)
        y_pred_w1 = X_c @ self.w1_true
        y_pred_w2 = X_c @ self.w2_true

        y[w1_mask] = y_pred_w1[w1_mask]
        y[~w1_mask] = y_pred_w2[~w1_mask]

        noise = np.random.randn(self.N) * self.noise_level
        y = y + noise

        return y, assignments

    def generate_data(self):
        X = self._generate_features()
        y, assignments = self._generate_labels(X)
        return X, y, assignments

    def get_true_weights(self):
        return self.w1_true, self.w2_true


class LinearRegressionAnalyzer:
    """
    Analyzes linear regression data
    """
    def __init__(self, X, y, assignments, w1_true, w2_true):
        self.X = X
        self.y = y
        self.assignments = assignments
        self.w1_true = w1_true
        self.w2_true = w2_true # Store w2_true as well for potential future analysis

        # Add intercept column to X once for reuse in analysis methods
        self.X_with_intercept = np.c_[np.ones(self.N), self.X]

    @property
    def N(self):
        return self.X.shape[0]

    @property
    def D(self):
        return self.X.shape[1]


    def _get_y1_data(self):
        """
        Filters the data to include only samples generated by w1 (assignments == 1).
        """
        y1_mask = self.assignments == 1
        X_y1 = self.X[y1_mask]
        y_y1 = self.y[y1_mask]
        return X_y1, y_y1

    def find_regression_solution_y1(self):
        X_y1, y_y1 = self._get_y1_data()

        # Add a column of ones for the intercept
        X_y1_with_intercept = np.c_[np.ones(X_y1.shape[0]), X_y1]

        # Calculate the normal equation solution
        XTX = X_y1_with_intercept.T @ X_y1_with_intercept
        XTy = X_y1_with_intercept.T @ y_y1
        w1_estimated = np.linalg.solve(XTX, XTy)

        return w1_estimated

    def find_regression_solution_all_data_normal_equation(self):
        # Use X_with_intercept already stored
        X_all_with_intercept = self.X_with_intercept
        y_all = self.y

        # Calculate the normal equation solution
        w_combined_estimated = np.linalg.inv(X_all_with_intercept.T @ X_all_with_intercept) @ (X_all_with_intercept.T @ y_all)

        return w_combined_estimated

    def find_regression_solution_gd(self, learning_rate: float, n_iterations: int, initial_w = None):
        X_all_with_intercept = self.X_with_intercept
        y_all = self.y
        N = self.N

        # Initialize w
        if initial_w is None:
            w = np.zeros(self.D + 1)
        else:
            if initial_w.shape != (self.D + 1,):
                raise ValueError(f"Initial weights must have shape ({self.D+1},).")
            w = initial_w.copy()

        loss_history = []

        # Gradient Descent loop
        for i in range(n_iterations):
            # predictions
            y_pred = X_all_with_intercept @ w

            # the error
            error = y_pred - y_all

            # the gradient of the MSE loss (1/N * ||y - Xw||^2)
            # (1/N) * X_transpose * (Xw - y) or  1/N * Sum(xi * (xi*w - yi))
            gradient = (1/N) * X_all_with_intercept.T @ error

            # Update weights
            w = w - learning_rate * gradient

            # Calculate and store the current MSE loss
            current_loss = np.mean(error**2)
            loss_history.append(current_loss)

        return w, loss_history

    def measure_weight_estimation_error(self, w_estimated, w_true):
        """
        Measures the L2 norm/Euclidean distance between the estimated weights and a specified true weights vector.
        """
        error = np.linalg.norm(w_estimated - w_true)
        return error

    def measure_prediction_error(self, w_estimated, X_data, y_data):
        """
        Measures MSE of predictions made by an estimated weight vector on given data.
        """
        # Add a column of ones for the intercept for prediction
        X_data_with_intercept = np.c_[np.ones(X_data.shape[0]), X_data]

        y_predicted = X_data_with_intercept @ w_estimated
        mse = np.mean((y_data - y_predicted)**2)
        return mse


    def analyze_y1_regression(self):
        print("\nAnalysis on y1 Data (Normal Equation - Performance Upper Bound for w1) ")
        w1_estimated = self.find_regression_solution_y1()
        weight_estimation_error = self.measure_weight_estimation_error(w1_estimated, self.w1_true)

        X_y1, y_y1 = self._get_y1_data()
        prediction_error_mse = self.measure_prediction_error(w1_estimated, X_y1, y_y1)


        print("Estimated w1 (using only y1 data):")
        if self.D + 1 > 10:
            print(np.round(w1_estimated[:5], 4), "...")
        else:
            print(np.round(w1_estimated, 4))

        print(f"\nWeight Estimation Error (L2 norm ||w1_estimated - w1_true||): {weight_estimation_error:.2f}")
        print(f"Prediction Error (MSE on y1 data): {prediction_error_mse:.2f}")

        return w1_estimated, weight_estimation_error, prediction_error_mse

    def analyze_all_data_regression_normal_equation(self):
        print("\n Analysis on All Data Samples (Normal Equation) ")
        w_combined_estimated = self.find_regression_solution_all_data_normal_equation()
        prediction_error_mse = self.measure_prediction_error(w_combined_estimated, self.X, self.y)
        distance_to_w1_true = self.measure_weight_estimation_error(w_combined_estimated, self.w1_true)
        distance_to_w2_true = self.measure_weight_estimation_error(w_combined_estimated, self.w2_true)


        print("Estimated w (using all data - Normal Equation):")
        if self.D + 1 > 10:
            print(np.round(w_combined_estimated[:5], 4), "...")
        else:
            print(np.round(w_combined_estimated, 4))

        print(f"\nPrediction Error (MSE on all data): {prediction_error_mse:.2f}")
        print(f"Distance of estimated w to w1_true (L2 norm): {distance_to_w1_true:.2f}")
        print(f"Distance of estimated w to w2_true (L2 norm): {distance_to_w2_true:.2f}")

        return w_combined_estimated, prediction_error_mse, distance_to_w1_true, distance_to_w2_true

    def analyze_gd_regression(self, learning_rate, n_iterations, initial_w = None, plot_loss = True):
        print("\n Analysis on All Data Samples (Gradient Descent) ")
        print(f"  Parameters: Learning Rate={learning_rate}, Iterations={n_iterations}")

        w_gd_estimated, loss_history = self.find_regression_solution_gd(
            learning_rate=learning_rate,
            n_iterations=n_iterations,
            initial_w=initial_w
        )

        prediction_error_mse = self.measure_prediction_error(w_gd_estimated, self.X, self.y)
        distance_to_w1_true = self.measure_weight_estimation_error(w_gd_estimated, self.w1_true)
        distance_to_w2_true = self.measure_weight_estimation_error(w_gd_estimated, self.w2_true)


        print("Estimated w (using all data - Gradient Descent):")
        if self.D + 1 > 10:
            print(np.round(w_gd_estimated[:5], 4), "...")
        else:
            print(np.round(w_gd_estimated, 4))

        print(f"\nFinal Prediction Error (MSE on all data): {prediction_error_mse:.2f}")
        print(f"Distance of estimated w to w1_true (L2 norm): {distance_to_w1_true:.2f}")
        print(f"Distance of estimated w to w2_true (L2 norm): {distance_to_w2_true:.2f}")

        if plot_loss:
            plt.figure(figsize=(10, 6))
            plt.plot(loss_history)
            plt.xlabel('Iteration')
            plt.ylabel('MSE Loss')
            plt.title(f'GD Loss Convergence (LR={learning_rate}, Iters={n_iterations})')
            plt.grid(True)
            plt.show()


        return w_gd_estimated, loss_history, prediction_error_mse, distance_to_w1_true, distance_to_w2_true

if __name__ == "__main__":
    print("**********************************************************************")
    print("Q2 ...")
    print("Step 1- Generating Data ...")
    # parameters for data generation
    N_samples = 1000  # N >= 1000
    D_features = 4   # D >= 4
    p1_prob = 0.9     # Probability of using w1
    noise = 0.5       # noise level

    # Generate the data
    data_generator = TwoModelDataGenerator(
        N=N_samples,
        D=D_features,
        p1=p1_prob,
        noise_level=noise
    )
    X_data, y_data, assignments = data_generator.generate_data()
    w1_true, w2_true = data_generator.get_true_weights()

    # Print results
    print(f"Generated {N_samples} samples with {D_features} features.")
    print(f"Using w1 with probability {p1_prob*100}% and w2 with probability {(1-p1_prob)*100}%.")
    print(f"Number of samples generated using w1: {np.sum(assignments == 1)}")
    print(f"Number of samples generated using w2: {np.sum(assignments == 0)}")
    print(f"True weights w1 shape: {w1_true.shape}, w2 shape: {w2_true.shape}")
    print("**********************************************************************")

    # Perform Analyses on generated data and the true weights
    print("Step 2- Perform Analyses on generated data ...")
    analyzer = LinearRegressionAnalyzer(X=X_data, y=y_data, assignments=assignments, w1_true=w1_true, w2_true=w2_true)

    # 1. Regression Analysis on y1 data only
    print("Regression Analysis on y1 data only ...")
    w1_estimated_y1, weight_error_y1, prediction_error_y1 = analyzer.analyze_y1_regression()
    print("**********************************************************************")

    # 2. Regression Analysis on all data samples
    print("Regression Analysis on all data samples ...")
    w_combined_estimated_ne, prediction_error_all_ne, dist_to_w1_ne, dist_to_w2_ne = analyzer.analyze_all_data_regression_normal_equation()
    print("**********************************************************************")

    # 3. Gradient Descent) Analysis on all data samples
    print("Gradient Descent) Analysis on all data samples ...")

    # 1: Reasonable learning rate and iterations
    w_gd_est1, loss_hist1, mse_gd1, dist_to_w1_gd1, dist_to_w2_gd1 = analyzer.analyze_gd_regression(
        learning_rate=0.005, # Adjust based on data scale and N, D
        n_iterations=5000,
        plot_loss=True
    )

    # 2: Smaller learning rate, more iterations
    w_gd_est2, loss_hist2, mse_gd2, dist_to_w1_gd2, dist_to_w2_gd2 = analyzer.analyze_gd_regression(
        learning_rate=0.001,
        n_iterations=10000,
        plot_loss=True
    )

    # 3: Larger learning rate (might overshoot or diverge)
    # Observe how the loss plot behaves
    w_gd_est3, loss_hist3, mse_gd3, dist_to_w1_gd3, dist_to_w2_gd3 = analyzer.analyze_gd_regression(
        learning_rate=0.02,
        n_iterations=2000,
        plot_loss=True
    )

    # Plot estimated_w agaisnt w_truth
    colors = ['b', 'g', 'r', 'c', 'm']
    color_index = 0
    plt.figure(figsize=(10, 6))
    plt.plot(w1_true, w1_estimated_y1, marker='+', linestyle='-', color= colors[color_index], label='regression only y1')
    color_index += 1
    plt.plot(w1_true, w_combined_estimated_ne, marker='+', linestyle='-', color= colors[color_index], label='regression y1, y2')

    plt.title('estimated_weights against true_weights')
    plt.xlabel('true_weights')
    plt.ylabel('estimated_weights')
    plt.legend()
    plt.show()